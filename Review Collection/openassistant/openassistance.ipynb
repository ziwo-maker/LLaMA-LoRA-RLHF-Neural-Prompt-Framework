{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8355021-4aca-4b55-8ab7-d2bba5529fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs initialized\n",
    "!git clone https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b89d6-93db-49a0-915d-f8b212e25491",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "\n",
    "!yum install git-lfs\n",
    "\n",
    "!git lfs install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a28213-1b99-4279-8890-3f9535a1c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1475679e-0ad3-4106-a4f9-8769c8b4a9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007858514785766602,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb9f608b343476885a30eeccac0cc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./oasst-sft-4-pythia-12b-epoch-3.5\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./oasst-sft-4-pythia-12b-epoch-3.5\")\n",
    "model.half()\n",
    "model = model.cuda()\n",
    "print(\"model loaded.\")\n",
    "\n",
    "\n",
    "def infer(prompt:str):\n",
    "    prompt = f'<|prompter|>{prompt}<|endoftext|><|assistant|>'\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids,\n",
    "                                    max_length=100,\n",
    "                                    num_beams=1,\n",
    "                                    temperature=0.9,\n",
    "                                    top_p=0.9)\n",
    "    response = tokenizer.decode(output_ids[0].cpu(), skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 注意，由于显存约束，模型能处理的长度有限，这里每次只是单轮对话。\n",
    "    # 多轮对话的格式\n",
    "    #   <|prompter|>some text<|endoftext|><|assistant|>some response<|endoftext|><|prompter|>some text...\n",
    "    # 注意到tokenizer的speical_tokens里有<|system|>，猜想功能应该和ChatGPT差不多，不过我没有测\n",
    "    \n",
    "    \n",
    "    prompt = \"您好\"\n",
    "    \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a2560-d6c2-4c91-8797-63cf2ad701d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def do_generation():\n",
    "    \n",
    "    with open('data/generation/dataclean.json','r') as f:\n",
    "        data_all=json.load(f)\n",
    "    ans=[];\n",
    "    path='./generation_gpt_saved.json'\n",
    "    right=0;\n",
    "    wrong=0\n",
    "    for i in data_all:\n",
    "        \n",
    "        s=i['question']\n",
    "        tmp = infer(s)\n",
    "        print(tmp)\n",
    "        ans.append(tmp)\n",
    "        \n",
    "#         print(\"正确答案是\"+i['groundTruth'][0])\n",
    "#         if(i['groundTruth'][0] in tmp):\n",
    "#             right=right+1\n",
    "#             print(\"此题正确\")\n",
    "#         else:\n",
    "#             wrong=wrong+1\n",
    "#             print(\"此题错误\")\n",
    "#     print(right*1.0/(right+wrong))\n",
    "#     print(right+wrong)\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(ans,f);\n",
    "        \n",
    "do_generation();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce4d35-6519-4782-90f5-aa41740f5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd165ea0-1289-4def-97f7-d06b0d848e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
